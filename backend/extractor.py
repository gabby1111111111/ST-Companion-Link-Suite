"""
Companion-Link å°çº¢ä¹¦æ•°æ®æå–å¼•æ“

ä½¿ç”¨ httpx + BeautifulSoup ä»å°çº¢ä¹¦é¡µé¢æå–ç»“æ„åŒ–æ•°æ®
å‚è€ƒ xiaohongshu-mcp çš„ service.go ä¸­çš„æ•°æ®æŠ“å–é€»è¾‘
"""

import re
import logging
from typing import Optional
from urllib.parse import urlparse

import httpx
from bs4 import BeautifulSoup

from config import settings
from models import NoteData, NoteAuthor, NoteComment, NoteInteraction

logger = logging.getLogger("companion-link.extractor")


class XiaohongshuExtractor:
    """
    å°çº¢ä¹¦ç¬”è®°æ•°æ®æå–å™¨

    æ”¯æŒä¸¤ç§æå–æ¨¡å¼ï¼š
    1. ç›´æ¥ä» URL æŠ“å–é¡µé¢ HTML â†’ è§£æ DOM
    2. è§£æé¡µé¢ä¸­çš„ __INITIAL_STATE__ JSONï¼ˆé¦–é€‰ï¼Œæ•°æ®æ›´å®Œæ•´ï¼‰
    """

    # å°çº¢ä¹¦ç¬”è®° URL æ­£åˆ™
    NOTE_URL_PATTERNS = [
        # https://www.xiaohongshu.com/explore/ç¬”è®°ID
        re.compile(r"xiaohongshu\.com/explore/([a-f0-9]+)"),
        # https://www.xiaohongshu.com/discovery/item/ç¬”è®°ID
        re.compile(r"xiaohongshu\.com/discovery/item/([a-f0-9]+)"),
        # https://xhslink.com/xxxxx (çŸ­é“¾)
        re.compile(r"xhslink\.com/([a-zA-Z0-9]+)"),
    ]

    def __init__(self):
        self.client = httpx.AsyncClient(
            timeout=settings.extract_timeout,
            headers={
                "User-Agent": settings.user_agent,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Referer": "https://www.xiaohongshu.com/",
            },
            follow_redirects=True,
        )

    async def close(self):
        """å…³é—­ HTTP å®¢æˆ·ç«¯"""
        await self.client.aclose()

    # ============================================================
    # å…¬å¼€æ¥å£
    # ============================================================

    def parse_note_id(self, url: str) -> Optional[str]:
        """ä»ç¬”è®° URL ä¸­è§£æå‡º note_id"""
        for pattern in self.NOTE_URL_PATTERNS:
            match = pattern.search(url)
            if match:
                return match.group(1)
        return None

    async def extract(self, note_url: str) -> NoteData:
        """
        æå–ç¬”è®°å®Œæ•´æ•°æ®

        Args:
            note_url: å°çº¢ä¹¦ç¬”è®° URL

        Returns:
            NoteData: ç»“æ„åŒ–ç¬”è®°æ•°æ®
        """
        note_id = self.parse_note_id(note_url) or ""

        # ç¡®ä¿ URL æ˜¯å®Œæ•´çš„ explore æ ¼å¼
        canonical_url = self._build_canonical_url(note_id, note_url)

        logger.info(f"ğŸ“¥ å¼€å§‹æå–ç¬”è®°: {canonical_url}")

        try:
            response = await self.client.get(canonical_url)
            response.raise_for_status()
            html = response.text
        except httpx.HTTPError as e:
            logger.error(f"âŒ HTTP è¯·æ±‚å¤±è´¥: {e}")
            return NoteData(
                note_id=note_id,
                note_url=note_url,
                title="[æå–å¤±è´¥]",
                content=f"æ— æ³•è·å–ç¬”è®°å†…å®¹: {str(e)}",
                content_summary="æå–å¤±è´¥",
            )

        # ä¼˜å…ˆä» __INITIAL_STATE__ æå–ï¼ˆæ•°æ®æ›´å®Œæ•´ï¼‰
        note_data = self._extract_from_initial_state(html, note_id, note_url)
        if note_data:
            logger.info(f"âœ… ä» __INITIAL_STATE__ æå–æˆåŠŸ: {note_data.title}")
            return note_data

        # é™çº§åˆ° DOM è§£æ
        note_data = self._extract_from_dom(html, note_id, note_url)
        logger.info(f"âœ… ä» DOM è§£ææå–æˆåŠŸ: {note_data.title}")
        return note_data

    # ============================================================
    # æå–ç­–ç•¥ 1: __INITIAL_STATE__ JSON
    # ============================================================

    def _extract_from_initial_state(
        self, html: str, note_id: str, note_url: str
    ) -> Optional[NoteData]:
        """
        ä»é¡µé¢çš„ __INITIAL_STATE__ è„šæœ¬ä¸­æå– JSON æ•°æ®
        å°çº¢ä¹¦ SSR é¡µé¢ä¼šå°†å®Œæ•´çš„ç¬”è®°æ•°æ®åµŒå…¥åˆ°è¿™ä¸ªå…¨å±€å˜é‡ä¸­
        """
        try:
            # åŒ¹é… window.__INITIAL_STATE__ = {...}
            pattern = re.compile(
                r"window\.__INITIAL_STATE__\s*=\s*(\{.+?\})\s*(?:;|</script>)",
                re.DOTALL,
            )
            match = pattern.search(html)
            if not match:
                return None

            import json

            # å°çº¢ä¹¦çš„ JSON ä¸­å¯èƒ½åŒ…å« undefinedï¼Œéœ€è¦æ›¿æ¢
            raw_json = match.group(1)
            raw_json = raw_json.replace("undefined", "null")
            data = json.loads(raw_json)

            # å¯¼èˆªåˆ°ç¬”è®°æ•°æ®èŠ‚ç‚¹
            note_detail = self._navigate_json(data, note_id)
            if not note_detail:
                return None

            # æå–å„å­—æ®µï¼ˆç”¨ or "" å…œåº•ï¼Œå› ä¸º JSON ä¸­å¯èƒ½æœ‰ null å€¼ï¼‰
            title = note_detail.get("title", "") or ""
            desc = note_detail.get("desc", "") or ""
            note_type = note_detail.get("type", "normal") or "normal"

            # ä½œè€…ä¿¡æ¯
            user_info = note_detail.get("user", {})
            author = NoteAuthor(
                user_id=user_info.get("userId", ""),
                nickname=user_info.get("nickname", ""),
                avatar_url=user_info.get("avatar", ""),
            )

            # äº’åŠ¨æ•°æ®
            interact_info = note_detail.get("interactInfo", {})
            interaction = NoteInteraction(
                like_count=self._safe_int(interact_info.get("likedCount", 0)),
                collect_count=self._safe_int(
                    interact_info.get("collectedCount", 0)
                ),
                comment_count=self._safe_int(
                    interact_info.get("commentCount", 0)
                ),
                share_count=self._safe_int(
                    interact_info.get("shareCount", 0)
                ),
            )

            # å›¾ç‰‡åˆ—è¡¨
            images = []
            for img in note_detail.get("imageList", []):
                url = img.get("urlDefault", "") or img.get("url", "")
                if url:
                    images.append(url)

            # æ ‡ç­¾
            tag_list = note_detail.get("tagList", [])
            tags = [t.get("name", "") for t in tag_list if t.get("name")]

            # çƒ­é—¨è¯„è®º
            top_comments = self._extract_comments_from_json(data, note_id)

            # ç”Ÿæˆæ‘˜è¦
            content_summary = self._make_summary(desc)

            return NoteData(
                note_id=note_id,
                note_url=note_url,
                title=title,
                content=desc,
                content_summary=content_summary,
                author=author,
                interaction=interaction,
                top_comments=top_comments,
                tags=tags,
                images=images,
                note_type=note_type,
            )

        except Exception as e:
            logger.warning(f"âš ï¸ __INITIAL_STATE__ è§£æå¤±è´¥: {e}")
            return None

    def _navigate_json(self, data: dict, note_id: str) -> Optional[dict]:
        """å¯¼èˆªåˆ°ç¬”è®°è¯¦æƒ…æ•°æ®èŠ‚ç‚¹"""
        # å°è¯•å¤šç§è·¯å¾„ï¼ˆå°çº¢ä¹¦å‰ç«¯ç»“æ„å¯èƒ½å˜åŒ–ï¼‰
        paths = [
            lambda d: d.get("note", {}).get("noteDetailMap", {})
                       .get(note_id, {}).get("note"),
            lambda d: d.get("note", {}).get("note"),
            lambda d: d.get("noteDetail", {}).get("data", {}).get("noteData"),
        ]
        for path_fn in paths:
            try:
                result = path_fn(data)
                if result and isinstance(result, dict):
                    return result
            except (KeyError, TypeError, AttributeError):
                continue
        return None

    def _extract_comments_from_json(
        self, data: dict, note_id: str
    ) -> list[NoteComment]:
        """ä» __INITIAL_STATE__ ä¸­æå–è¯„è®º"""
        comments = []
        max_count = settings.extract_max_comments

        try:
            # è¯„è®ºæ•°æ®å¯èƒ½åœ¨ä¸åŒè·¯å¾„
            comment_paths = [
                lambda d: d.get("comment", {}).get("comments", []),
                lambda d: d.get("note", {}).get("noteDetailMap", {})
                           .get(note_id, {}).get("comments", []),
            ]

            raw_comments = []
            for path_fn in comment_paths:
                try:
                    result = path_fn(data)
                    if result:
                        raw_comments = result
                        break
                except (KeyError, TypeError):
                    continue

            # æŒ‰ç‚¹èµæ’åºå– TOP N
            sorted_comments = sorted(
                raw_comments,
                key=lambda c: self._safe_int(c.get("likeCount", 0)),
                reverse=True,
            )

            for c in sorted_comments[:max_count]:
                user_info = c.get("userInfo", {})
                comments.append(
                    NoteComment(
                        user_nickname=user_info.get("nickname", "åŒ¿åç”¨æˆ·"),
                        content=c.get("content", ""),
                        like_count=self._safe_int(c.get("likeCount", 0)),
                        sub_comment_count=self._safe_int(
                            c.get("subCommentCount", 0)
                        ),
                    )
                )
        except Exception as e:
            logger.warning(f"âš ï¸ è¯„è®ºæå–å¤±è´¥: {e}")

        return comments

    # ============================================================
    # æå–ç­–ç•¥ 2: DOM è§£æï¼ˆé™çº§æ–¹æ¡ˆï¼‰
    # ============================================================

    def _extract_from_dom(
        self, html: str, note_id: str, note_url: str
    ) -> NoteData:
        """é€šè¿‡ BeautifulSoup è§£æ HTML DOM"""
        soup = BeautifulSoup(html, "lxml")

        # æ ‡é¢˜
        title = ""
        title_el = soup.select_one("#detail-title") or soup.select_one(
            ".title, .note-title"
        )
        if title_el:
            title = title_el.get_text(strip=True)

        # æ­£æ–‡
        content = ""
        content_el = soup.select_one("#detail-desc") or soup.select_one(
            ".desc, .note-content, .content"
        )
        if content_el:
            content = content_el.get_text(strip=True)

        # og æ ‡ç­¾é™çº§
        if not title:
            og_title = soup.select_one('meta[property="og:title"]')
            if og_title:
                title = og_title.get("content", "")
        if not content:
            og_desc = soup.select_one('meta[property="og:description"]')
            if og_desc:
                content = og_desc.get("content", "")

        # ä½œè€…
        author = NoteAuthor()
        author_el = soup.select_one(".author .name, .user-nickname")
        if author_el:
            author.nickname = author_el.get_text(strip=True)

        # äº’åŠ¨æ•°æ®
        interaction = NoteInteraction()
        like_el = soup.select_one(
            '.like-wrapper .count, [data-type="like"] .count'
        )
        if like_el:
            interaction.like_count = self._safe_int(
                like_el.get_text(strip=True)
            )

        # è¯„è®º
        top_comments = []
        comment_els = soup.select(".comment-item, .parent-comment")
        for cel in comment_els[: settings.extract_max_comments]:
            nick_el = cel.select_one(".name, .user-name")
            text_el = cel.select_one(
                ".content, .comment-text, .note-text"
            )
            like_el = cel.select_one(".like .count, .like-count")
            if text_el:
                top_comments.append(
                    NoteComment(
                        user_nickname=(
                            nick_el.get_text(strip=True) if nick_el else "åŒ¿å"
                        ),
                        content=text_el.get_text(strip=True),
                        like_count=(
                            self._safe_int(like_el.get_text(strip=True))
                            if like_el
                            else 0
                        ),
                    )
                )

        # æ ‡ç­¾
        tags = []
        tag_els = soup.select(".tag, .hashtag, a[href*='tag']")
        for t in tag_els:
            text = t.get_text(strip=True).lstrip("#")
            if text:
                tags.append(text)

        return NoteData(
            note_id=note_id,
            note_url=note_url,
            title=title or "[æ— æ ‡é¢˜]",
            content=content,
            content_summary=self._make_summary(content),
            author=author,
            interaction=interaction,
            top_comments=top_comments,
            tags=tags,
        )

    # ============================================================
    # å·¥å…·æ–¹æ³•
    # ============================================================

    def _build_canonical_url(self, note_id: str, fallback_url: str) -> str:
        """æ„å»ºæ ‡å‡†åŒ–ç¬”è®° URL"""
        if note_id:
            return f"https://www.xiaohongshu.com/explore/{note_id}"
        return fallback_url

    def _make_summary(self, text: str) -> str:
        """ç”Ÿæˆæ­£æ–‡æ‘˜è¦"""
        if not text:
            return ""
        max_len = settings.extract_summary_length
        cleaned = re.sub(r"\s+", " ", text).strip()
        if len(cleaned) <= max_len:
            return cleaned
        return cleaned[:max_len] + "..."

    @staticmethod
    def _safe_int(value) -> int:
        """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¤„ç† '1.2ä¸‡' ç­‰ä¸­æ–‡æ•°å­—"""
        if isinstance(value, int):
            return value
        if isinstance(value, float):
            return int(value)
        if isinstance(value, str):
            value = value.strip()
            if not value:
                return 0
            # å¤„ç† "1.2ä¸‡", "3ä¸‡" ç­‰
            wan_match = re.match(r"([\d.]+)\s*ä¸‡", value)
            if wan_match:
                return int(float(wan_match.group(1)) * 10000)
            # å¤„ç†çº¯æ•°å­—
            try:
                return int(re.sub(r"[^\d]", "", value) or 0)
            except ValueError:
                return 0
        return 0


# å…¨å±€æå–å™¨å®ä¾‹
extractor = XiaohongshuExtractor()
